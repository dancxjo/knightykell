[hosts]

[hosts.brainstem]
image = "rpi"
services = ["voice"]
[hosts.brainstem.hrs04]
trig_pin = 17
echo_pin = 27

[hosts.cerebellum]
image = "ubuntu"
model="pi5"
services = ["voice", "logticker", "display", "topics", "notify", "fortune", "vision", "asr", "create", "singer"]
[hosts.cerebellum.display]
topics = ["/logs", "/topics", "/status/notify", "/vision/faces", "/vision/objects", "/vision/face_id"]
driver = "sh1106"
width = 128
height = 64
port = 1
address = "0x3C"
rotate = 0
h_offset = 2
extra = ""  # optional extra bottom-line text
[hosts.cerebellum.asr]
model = "tiny"

[hosts.cerebellum.create]
# AutonomyLab Create 1 bringup (via create_bringup)
package = "create_bringup"
launch_file = "create_1.launch"
# Use 'params.dev' for the serial device as expected by create_bringup
params = { dev = "/dev/ttyUSB0" }
# Additional launch args (optional): desc:=false to skip robot description
# args = ["desc:=false"]

[hosts.cerebellum.singer]
port = "/dev/ttyUSB0"  # reuse create port
baud = 57600
period = 60

[hosts.cerebellum.fortune]
# Emit a fortune every 5 minutes and also show a desktop notification if available
period = 300
notify_send = true

[hosts.forebrain]
services = ["voice", "logticker", "chat", "asr_long"]
[hosts.forebrain.asr_long]
model = "large-v2"

[hosts.forebrain.chat]
# Optional system prompt that seeds the conversation
prompt = "You are a robot named Pete Knightykell."
backend = "llama"  # prefer local llama-cpp-python over Ollama
# timeout = 30
gguf_url = "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q4_K_M.gguf"


[hosts.latimer]
services = []
[hosts.cerebellum.vision]
device = 0
width = 640
height = 480
fps = 5
